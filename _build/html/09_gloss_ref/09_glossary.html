
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Glossary &#8212; Remote Camera Decision Support Tool - Concept Library</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=32062c0a" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = '09_gloss_ref/09_glossary';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="References" href="08_references.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Glossary</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!--<style>
  h2 {
    font-weightnormal;
    color#2F5496
  }
</style>-->
<section class="tex2jax_ignore mathjax_ignore" id="glossary">
<span id="id1"></span><h1>Glossary<a class="headerlink" href="#glossary" title="Link to this heading">#</a></h1>
<p>Bold indicates a metadata field in either the <a class="reference external" href="https://ab-rcsc.github.io/RCSC-WildCAM_Remote-Camera-Survey-Guidelines-and-Metadata-Standards/2_metadata-standards/2_0.1_Citation-and-Info.html">AB Metadata Standards</a> (RCSC, 2024) or <a class="reference external" href="https://ab-rcsc.github.io/RCSC-WildCAM_Remote-Camera-Survey-Guidelines-and-Metadata-Standards/1_survey-guidelines/1_0.1_Citation-and-Info.html">Remote Camera Survey Guidelines</a> (RCSC et al., 2024).<br>
An asterisk (*) indicates the field is optional and not required by the AB Metadata Standards (RCSC, 2024) and B.C. Metadata Standards (RISC, 2019).</p>
<p id="access-method"><strong>*Access Method</strong></p>
<p>The method used to reach the camera location (e.g., on “Foot,” “ATV,” “Helicopter,” etc.).</p>
<p id="age-class-adult"><strong>Adult</strong></p>
<p>Animals that are old enough to breed; reproductively mature.</p>
<p id="age-class"><strong>Age Class</strong></p>
<p>The age classification of individual(s) being categorized (e.g., “Adult,” “Juvenile,” “Subadult,” “Subadult - Young of Year,” “Subadult - Yearling”, or “Unknown”).</p>
<p id="analyst"><strong>Analyst</strong></p>
<p>The first and last names of the individual who provided the observation data point (species identification and associated information). If there are multiple analysts for an observation, enter the primary analyst.</p>
<p id="animal-id"><strong>*Animal ID</strong></p>
<p>A unique ID for an animal that can be uniquely identified (e.g., marked in some way). If multiple unique individuals are identified, enter an Animal ID for each as a unique row. Leave blank if not applicable.</p>
<p id="baitlure-audible-lure">Audible lure</p>
<p>Sounds imitating noises of prey or conspecifics that draw animals closer by eliciting curiosity (Schlexer, 2008).</p>
<p id="baitlure-bait">Bait</p>
<p>A food item (or other substance) that is placed to attract animals via the sense of taste and olfactory cues (Schlexer, 2008).</p>
<p id="baitlure-bait-lure-type"><strong>Bait/Lure Type</strong></p>
<p>The type of bait or lure used at a camera location. Record “None” if a Bait/Lure Type was not used and “Unknown” if not known. If “Other,” describe in the Deployment Comments.</p>
<p id="batteries-replaced"><strong>*Batteries Replaced</strong></p>
<p>Whether the camera’s batteries were replaced.</p>
<p id="behaviour"><strong>*Behaviour</strong></p>
<p>The behaviour of the individual(s) being categorized (e.g., “Standing,” “Drinking,” “Vigilant,” etc.).</p>
<p id="camera-active-on-arrival"><strong>*Camera Active On Arrival</strong></p>
<p>Whether a camera was functional upon arrival.</p>
<p id="camera-active-on-departure"><strong>*Camera Active On Departure</strong></p>
<p>Whether a camera was functional upon departure.</p>
<p id="camera-angle">Camera angle</p>
<p>The degree at which the camera is pointed toward the FOV Target Feature relative to the horizontal ground surface (with respect to slope, if applicable).</p>
<p id="camera-attachment"><strong>*Camera Attachment</strong></p>
<p>The method/tools used to attach the camera (e.g., attached to a tree with a bungee cord; reported as codes such as “Tree + Bungee/Strap”). If “Other,” describe in the Camera Location Comments.</p>
<p id="camera-damaged"><strong>*Camera Damaged</strong></p>
<p>Whether the camera was damaged or malfunctioning; if there is any damage to the device (physical or mechanical), the crew should describe the damage in the Service/Retrieval Comments.</p>
<p id="camera-days-per-camera-location">Camera days per camera location</p>
<p>The number of days each camera was active and functioning during the period it was deployed (e.g., 24-hour periods or the difference in days between the Deployment Start Date Time and the Deployment End Date Time if there were no interruptions).</p>
<p id="camera-direction"><strong>*Camera Direction (degrees)</strong></p>
<p>The cardinal direction that a camera faces. Ideally, cameras should face north (N; i.e. “0” degrees), or south (S; i.e. “180” degrees) if north is not possible. The Camera Direction should be chosen to ensure the field of view (FOV) is of the original FOV target feature.</p>
<p id="camera-height"><strong>Camera Height (m)</strong></p>
<p>The height from the ground (below snow) to the bottom of the lens (metres; to the nearest 0.05 m).</p>
<p id="camera-id"><strong>Camera ID</strong></p>
<p>A unique alphanumeric ID for the camera that distinguishes it from other cameras of the same make or model.</p>
<p id="camera-location">Camera location</p>
<p>The location where a single camera was placed (recorded as “Camera Location Name”).</p>
<p id="camera-location-characteristics"><strong>*Camera Location Characteristic(s)</strong></p>
<p>Any significant features around the camera at the time of the visit. This may include for example, manmade or natural linear features (e.g., trails), habitat types (e.g., wetlands), wildlife structure (e.g., beaver dam). If “Other,” describe in the Camera Location Comments.</p>
<p>Camera Location Characteristics differ from FOV Target Features in that Camera Location Characteristics could include those not in the camera’s Field of View. If “Other,” describe in the Camera Location Comments.</p>
<p id="camera-location-comments"><strong>*Camera Location Comments</strong></p>
<p>Comments describing additional details about a camera location.</p>
<p id="camera-location-name"><strong>Camera Location Name</strong></p>
<p>A unique alphanumeric identifier for the location where a single camera was placed (e.g., “bh1,” “bh2”).</p>
<p id="camera-make"><strong>Camera Make</strong></p>
<p>The make of a particular camera (i.e., the manufacturer, e.g., “Reconyx” or “Bushnell”).</p>
<p id="camera-model"><strong>Camera Model</strong></p>
<p>The model number or name of a particular camera (e.g., “PC900” or “Trophy Cam HD”).</p>
<p id="camera-serial-number"><strong>Camera Serial Number</strong></p>
<p>The serial number of a particular camera, which is usually found inside the camera cover (e.g., “P900FF04152022”).</p>
<p id="camera-spacing">Camera spacing</p>
<p>The distance between cameras (i.e., also referred to as “inter-trap distance”). This will be influenced by the chosen sampling design, the Survey Objectives, the Target Species and data analysis.</p>
<p id="mods-cr-cmr">Capture-recapture (CR) model / Capture-mark-recapture (CMR) model (Karanth, 1995; Karanth &amp; Nichols, 1998)</p>
<p>A method of estimating the abundance or density of marked populations using the number of animals detected and the likelihood animals will be detected (detection probability). CR (Karanth, 1995; Karanth &amp; Nichols, 1998) can be used to estimate vital rates where all newly detected unmarked animals become marked and are distinguishable in future (Efford, 2022). Spatially explicit capture-recapture (SECR; Borchers &amp; Efford, 2008; Efford, 2004; Royle &amp; Young, 2008) models have largely replaced CR and CMR models and provide more accurate density estimates (Blanc et al., 2013, Obbard et al., 2010, Sollmann et al., 2011).</p>
<p id="mods-catspim">Categorical partial identity model (catSPIM) (Augustine et al., 2019; Sun et al., 2022)</p>
<p>A method used to estimate the density of partially marked populations in which the “spatial locations of where partial identity samples are captured to probabilistically resolve their complete identities” (Augustine et al., 2018, 2019). catSPIM models use partial identity traits (e.g., sex class, antler points) to help infer individual identities (Augustine et al., 2019; Sun et al., 2022). catSPIM is an extension of the SC model (Chandler &amp; Royle, 2013).</p>
<p id="sampledesign-clustered">Clustered design</p>
<p>Multiple cameras are deployed at a sample station (Figure 3d). A clustered design can be used within a systematic or stratified approach (i.e., systematic clustered design or as a clustered random design [Wearn &amp; Glover-Kapfer, 2017]).</p>
<p id="sampledesign-convenience">Convenience design</p>
<p>Camera locations or sample stations are chosen based on logistic considerations (e.g., remoteness, access constraints, and/or costs).</p>
<p id="crew">Crew</p>
<p>The first and last names of all the individuals who collected data during the deployment visit (“Deployment Crew”) and service/retrieval visit (“Service/Retrieval Crew”).</p>
<p id="cumulative-det-probability">Cumulative detection probability</p>
<p>The probability of detecting a species at least once during the entire survey (Steenweg et al., 2019).</p>
<p id="density">Density</p>
<p>The number of individuals per unit area.</p>
<p id="deployment">Deployment</p>
<p>A unique placement of a camera in space and time (recorded as “Deployment Name”). There may be multiple deployments for one camera location. Deployments are often considered as the time between visits (i.e., deployment to service, service to service, and service to retrieval). Any change to camera location, sampling period, camera equipment (e.g., Trigger Sensitivity setting, becomes non-functioning), and/or conditions (e.g., not baited then baited later; camera SD card replaced) should be documented as a unique deployment.</p>
<p id="deployment-area-photo-numbers"><strong>*Deployment Area Photo Numbers</strong></p>
<p>The image numbers for the deployment area photos (if collected, e.g., “DSC100”). These are optionally documented on a Camera Deployment Field Datasheet for each set of camera deployment area photos. Leave blank if not applicable.</p>
<p id="deployment-area-photos">Deployment area photos</p>
<p>Photos of the area around the camera location, collected as a permanent, visual record of the FOV Target Features, Camera Location Characteristics, environmental conditions (e.g., vegetation, ecosite, weather) or other variables of interest. The recommendation includes collecting four photos taken from the centre of the target detection zone (Figure 5), facing each of the four cardinal directions. The documentation of the collection of these photos is recorded as “Deployment Area Photos Taken” (Y/N).</p>
<p id="deployment-area-photos-taken"><strong>*Deployment Area Photos Taken</strong></p>
<p>Whether deployment area photos were taken (yes/no; optional). The recommendation includes collecting four photos taken from the centre of the target detection zone (Figure 5), facing each of the four cardinal directions.</p>
<p id="deployment-comments"><strong>*Deployment Comments</strong></p>
<p>Comments describing additional details about the deployment.</p>
<p id="deployment-crew"><strong>Deployment Crew</strong></p>
<p>The first and last names of the individuals who collected data during the deployment visit.</p>
<p id="deployment-end-date-time"><strong>Deployment End Date Time (DD-MMM-YYYY HH:MM:SS)</strong></p>
<p>The date and time that the data was retrieved for a specific deployment (e.g., 27-Jan-2019 23:00:00). The Deployment End Date Time may not coincide with when the last image or video was collected (i.e., the Image Set End Date Time). Recording this field allows users to account for deployments where no images were captured and to confirm the last date and time that the camera was active.</p>
<p id="deployment-image-count"><strong>*Deployment Image Count</strong></p>
<p>The total number of images collected during the deployment, including false triggers (i.e., empty images with no wildlife or human present species) and those triggered by a time-lapse setting (if applicable).</p>
<p id="deployment-metadata">Deployment metadata</p>
<p>Metadata that is collected each time a camera is deployed. Each deployment event should have its own Camera Deployment Field Datasheet. The relevant metadata fields that should be collected differ when a camera is deployed vs. serviced or retrieved.\</p>
<p>Refer to Appendix A - Table A5 and Camera Deployment Field Datasheet.</p>
<p id="deployment-name"><strong>Deployment Name</strong></p>
<p>A unique alphanumeric identifier for a unique camera deployed during a specific survey period (ideally recorded as “Camera Location Name,”_”,Deployment Start Date” (or …*“Deployment End Date”) (e.g., “bh1_17-Jul-2018” or “bh1_17-Jul-2018_21-Jan-2019”).</p>
<p>Alternative naming conventions may be used, but the goal should be to minimize duplicate Image Names.</p>
<p id="deployment-start-date-time"><strong>Deployment Start Date Time (DD-MMM-YYYY HH:MM:SS)</strong></p>
<p>The date and time that a camera was placed for a specific deployment (e.g., 17-Jan-2018 10:34:22).</p>
<p>The Deployment Start Date Time may not coincide with when the first image or video was collected (i.e., the Image Set Start Date Time). Recording this field allows users to account for deployments where no images were captured and to confirm the first date and time a camera was active.</p>
<p id="deployment-visit">Deployment visit</p>
<p>When a crew has gone to a location to deploy a remote camera.</p>
<p id="detection-event">Detection “event”</p>
<p>A group of images or video clips that are considered independent from other images or video clips based on a certain time threshold (or “inter-detection interval”). For example, 30 minutes (O’Brien et al., 2003; Gerber et al., 2010; Kitamura et al., 2010; Samejima et al., 2012) or 1 hour (e.g., Tobler et al., 2008; Rovero &amp; Marshall, 2009).</p>
<p id="detection-distance">Detection distance</p>
<p>“The maximum distance that a sensor can detect a target” (Wearn and Glover-Kapfer, 2017).</p>
<p id="detection-probability">Detection probability (aka detectability)</p>
<p>The probability (likelihood) that an individual of the population of interest is included in the count at time or location <em>i</em>.</p>
<p id="detection-rate">Detection rate</p>
<p>The frequency of independent detections within a specified time period.</p>
<p id="detection-zone">Detection zone</p>
<p>The area (conical in shape) in which a remote camera can detect the heat signature and motion of an object (Rovero &amp; Zimmermann, 2016) (Figure 5).</p>
<p id="mods-distance-sampling">Distance sampling (DS) model (Howe et al., 2017)</p>
<p>A method to estimate abundance by using distances at which animals are detected (from survey lines or points) to model abundance as a function of decreasing detection probability with animal distance from the camera (using a decay function) (Cappelle et al., 2021; Howe et al., 2017).</p>
<p id="easting-camera-location"><strong>Easting Camera Location</strong></p>
<p>The easting UTM coordinate of the camera location (e.g., “337875”). Record using the NAD83 datum. Leave blank if recording the Longitude instead.</p>
<p id="effective-detection-distance">Effective detection distance</p>
<p>The distance from a camera that would give the same number of detections if all animals up to that distance are perfectly detected, and no animals that are farther away are detected; Buckland, 1987, Becker et al., 2022).</p>
<p id="event-type"><strong>Event Type</strong></p>
<p>Whether detections were reported as an individual image captured by the camera (“Image”), a “Sequence,” or “Tag.”</p>
<p id="false-trigger">False trigger</p>
<p>Blank images (no wildlife or human present). These images commonly occur when a camera is triggered by vegetation blowing in the wind.</p>
<p id="field-of-view">Field of View (FOV)</p>
<p>The extent of a scene that is visible in an image (Figure 5); a large FOV is obtained by “zooming out” from a scene, whilst “zooming in” will result in a smaller FOV (Wearn &amp; Glover-Kapfer, 2017).</p>
<p id="settings-flash-output">Flash output</p>
<p>The camera setting that provides the level of intensity of the flash (if enabled).</p>
<p id="fov-target"><strong>FOV Target Feature</strong></p>
<p>A specific man-made or natural feature at which the camera is aimed to maximize the detection of wildlife species or to measure the use of that feature. Record “None” if a FOV Target Feature was not used and “Unknown” if not known. If “Other,” describe in the Camera Location Comments.</p>
<p id="fov-target-distance"><strong>*FOV Target Feature Distance (m)</strong></p>
<p>The distance from the camera to the FOV Target Feature (in metres; to the nearest 0.5 m). Leave blank if not applicable.</p>
<p id="gps-unit-accuracy"><strong>GPS Unit Accuracy (m)</strong></p>
<p>The margin of error of the GPS unit used to record spatial information (e.g., “5” [m]), such as the coordinates of the camera location. On most GPS units (e.g., “Garmin”) this information is provided on the unit’s satellite information page.</p>
<p id="human-transport-mode-activity"><strong>*Human Transport Mode/Activity</strong></p>
<p>The activity performed or mode of transportation used by a human observed (e.g., hiker, skier, off-highway vehicle, etc.). This categorical field should be populated when data on humans (in addition to wildlife) are collected. Leave blank if not applicable and record “Unknown” if not known.</p>
<p id="mods-hurdle">Hurdle model (Mullahy, 1986; Heilbron 1994)</p>
<p>A regression model used in the setting of excess zeros (zero-inflation) and overdispersion (Mullahy, 1986). Hurdle models (aka “zero-altered” models) differ from zero-inflation models in that they are two-part models, and the zero and non-zero counts are modelling separately (thus, they are only adequate when the counting process cannot generate a zero value) (Blasco-Moreno et al., 2019). [relative abundance indices]</p>
<p id="image">Image</p>
<p>An individual image captured by a camera, which may be part of a multi-image sequence (recorded as “Image Name”).</p>
<p id="image-classification">Image classification</p>
<p>The process of assigning class labels to an image according to the wildlife species, other entities (e.g., human, vehicle), or conditions within the image. Image classification can be performed manually or automatically by an artificial intelligence (AI) algorithm. Image classification is sometimes used interchangeably with “image tagging.”</p>
<p id="image-classification-confidence">Image classification confidence</p>
<p>The likelihood of an image containing an object of a certain class (Fennell et al., 2022).</p>
<p id="image-flash-output"><strong>*Image Flash Output</strong></p>
<p>The Image Flash Output is an image metadata field indicating the level of intensity of the flash [if enabled/applicable]). Record as reported in the image Exif data (e.g., “Flash Did Not Fire”, “Auto”). This field is in text format; record “Unknown” if not known; leave blank if not applicable.</p>
<p id="image-infrared-illuminator"><strong>*Image Infrared Illuminator</strong></p>
<p>The Image Infrared Illuminator is an image metadata field indicating whether the infrared illuminator setting was enabled (if applicable; to obtain greater visibility at night by producing infrared light). Record as reported in the image Exif data (e.g., “On” or “Off”). This field is categorical; leave blank if not applicable and record “Unknown” if not known.</p>
<p id="image-name"><strong>Image Name</strong></p>
<p>A unique alphanumeric identifier for the image. It is important to include (at a minimum) the camera location, date, time, and image number when generating an Image Name to avoid duplicate file names (e.g., “bh1_17-Jul-2018_P900FF04152022_22-Jul-2018 10:34:22_img_100” or “bh1_17-Jul-2018_22-Jul-2018_10:34:22_img_100”).</p>
<p id="image-processing">Image processing</p>
<p>The series of operations that are taken to extract information from images. In the case of remote camera data, it can include loading the images into a processing platform, extracting information from the image metadata (e.g., the date and time the image was taken), running an artificial intelligence (AI) algorithm to identify empty images, classifying animals or other entities within the image.</p>
<p id="image-sequence">Image Sequence</p>
<p>The order of the image in a rapid-fire sequence as reported in the image Exif data (text; e.g., “1 of 1” or “1 of 3”). Leave blank if not applicable.</p>
<p id="image-set-end-date-time"><strong>Image Set End Date Time (DD-MMM-YYYY HH:MM:SS)</strong></p>
<p>The date and time of the last image or video collected during a specific deployment (e.g., “17-Jan-2018 22:10:05”).</p>
<p>The Image Set End Date Time may not coincide with the deployment end date time. Recording this field allows users to account for deployments that were conducted but for which no data was found and to confirm the last date and time a camera was active (if functioning) if no images or videos were captured prior to Service/Retrieval (especially valuable if users did not collect Time-lapse images or if the camera malfunctioned).</p>
<p id="image-set-start-date-time"><strong>Image Set Start Date Time (DD-MMM-YYYY HH:MM:SS)</strong></p>
<p>The date and time of the first image or video collected during a specific deployment (e.g., “17-Jan-2018 12:00:02”).</p>
<p>The Image Set Start Date Time may not coincide with the Deployment Start Date Time. Recording this field allows users to confirm the first date and time a camera was active (reliable if Time-lapse images were collected; especially valuable if the user scheduled a start delay).</p>
<p id="image-tagging">Image tagging</p>
<p>The process of classifying an image according to the wildlife species, other entities (e.g., human, vehicle), or conditions within the image. Image tagging may follow image classification to further classify characteristics of the individuals (e.g., age class, sex class, or behaviour) or entities within the image.</p>
<p id="image-trigger-mode"><strong>*Image Trigger Mode</strong></p>
<p>The type of trigger mode used to capture the image as reported in the image Exif data (e.g., “Time Lapse”, “Motion Detection,” “CodeLoc Not Entered,” “External Sensor”). Record “Unknown” if not known.</p>
<p id="image-sequence-comments"><strong>*Image/Sequence Comments</strong></p>
<p>Comments describing additional details about the image/sequence.</p>
<p id="image-sequence-date-time"><strong>Image/Sequence Date Time (DD-MMM-YYYY HH:MM:SS)</strong></p>
<p>The date and time of an image, or the image chosen to represent the sequence, recorded as “DD-MMM-YYYY HH:MM:SS” (e.g., 22-Jul-2018 11:02:02).</p>
<p>Sequence date/time information may be reported for a “representative image” of a sequence (i.e., the image with the most information). For example, if three images were included in a sequence, but the Sex Class could only be discerned in the second image [all else remaining equal], the second image would be the best representative image of the sequence.</p>
<p>The Image/Sequence Date Time differs from the Image Set Start Date Time which refers to the first image or video collected during a deployment.</p>
<p id="imperfect-detection">Imperfect detection</p>
<p>Species are often detected “imperfectly,” meaning that they are not always detected when they are present (e.g., due to cover of vegetation, cryptic nature or small size) (MacKenzie et al., 2004).</p>
<p id="independent-detections">Independent detections</p>
<p>Detections that are deemed to be independent based on a user-defined threshold (e.g., 30 minutes).</p>
<p id="individual-count"><strong>Individual Count</strong></p>
<p>The number of unique individuals being categorized. Depending on the Event Type, this may be recorded as the total number of individuals, or according to Age Class and/or Sex Class.</p>
<p id="settings-infrared-illum">Infrared illuminator</p>
<p>The camera setting that can be enabled (if applicable to the camera make and camera model) to obtain greater visibility at night by producing infrared light. This field is categorical; leave blank if not applicable and record “Unknown” if not known.</p>
<p id="mods-instantaneous-sampling">Instantaneous sampling (IS) (Moeller et al., 2018)</p>
<p>A method used to estimate abundance or density from time-lapse images from randomly deployed cameras; the number of unique individuals (the count) is needed (Moeller et al., 2018).</p>
<p id="intensity-of-use">Intensity of use (Keim et al., 2019)</p>
<p>“The expected number of use events of a specific resource unit during a unit of time… [which characterizes] how frequently a particular resource unit is used” (Keim et al., 2019). The intensity of use differs from the probability of use (which characterizes “the probability of at least one use event of that resource unit during a unit of time”; Keim et al., 2019).</p>
<p id="inter-detection-interval">Inter-detection interval</p>
<p>A user-defined threshold used to define a single “detection event” (i.e., independent “events”) for group of images or video clips (e.g., 30 minutes or 1 hour). The threshold should be recorded in the Survey Design Description.</p>
<p id="mods-inventory">Inventory</p>
<p>Rapid assessment surveys to determine what species are present in a given area at a given point in time; there is no attempt made to quantify aspects of communities or populations (Wearn &amp; Glover-Kapfer, 2017).</p>
<p id="age-class-juvenile"><strong>Juvenile</strong></p>
<p>Animals in their first summer, with clearly juvenile features (e.g., spots); mammals older than neonates but that still require parental care.</p>
<p id="kernel-density-estimator">Kernel density estimator</p>
<p>The probability of “utilization” (Jennrich &amp; Turner, 1969); describes the relative probability of use (Powell &amp; Mitchell, 2012).</p>
<p id="key-id"><strong>*Key ID</strong></p>
<p>The unique ID for the specific key or set of keys used to lock/secure the camera to the post, tree, etc.</p>
<p id="latitude-camera-location"><strong>Latitude Camera Location</strong></p>
<p>The latitude of the camera location in decimal degrees to five decimal places (e.g., “53.78136”). Leave blank if recording Northing instead.</p>
<p id="longitude-camera-location"><strong>Longitude Camera Location</strong></p>
<p>The longitude of the camera location in decimal degrees to five decimal places (e.g., “-113.46067”). Leave blank if recording Easting instead.</p>
<p id="baitlure-lure">Lure</p>
<p>Any substance that draws animals closer; lures include scent (olfactory) lure, visual lure and audible lure (Schlexer, 2008).</p>
<p id="typeid-marked">Marked individuals / populations / species</p>
<p>Individuals, populations, or species (varies with modelling approach and context) that can be identified using natural or artificial markings (e.g., coat patterns, scars, tags, collars).</p>
<p id="mods-mr">Mark-resight (MR) model (Arnason et al., 1991; McClintock et al., 2009)</p>
<p>A method used to estimate the abundance of partially marked populations using the number of marked individuals, the number of unmarked individuals, and the detection probability from marked animals (Wearn &amp; Glover-Kapfer, 2017). MR is similar to capture-recapture (CR; Karanth, 1995; Karanth &amp; Nichols, 1998) models, except only a portion of animals are individually identified.</p>
<p id="metadata">Metadata</p>
<p>Data that provides information about other data (e.g., the number of images on an SD card).</p>
<p id="mods-modelling-assumption">Model assumption</p>
<p>Explicitly stated (or implicitly premised) conventions, choices and other specifications (e.g., about the data, wildlife ecology/behaviour, the relationships between variables, etc.) on which a particular modelling approach is based that allows the model to provide valid inference.</p>
<p id="mods-modelling-approach">Modelling approach</p>
<p>The method used to analyze the camera data, which should depend on the state variable, e.g., occupancy models [MacKenzie et al., 2002], spatially explicit capture recapture (SECR) for density estimation [Chandler and Royle, 2013], etc. and the Target Species.</p>
<p id="settings-motion-image-interval"><strong>Motion Image Interval (seconds)</strong></p>
<p>The time (in seconds) between images within a multi-image sequence that occur due to motion, heat, or activation of external detector devices. The Motion Image Interval is pre-set in the camera’s settings by the user, but the time at which the camera collects images because of this setting is influenced by the presence of movement or heat. For example, if the camera was set to take 3 images per event at a Motion Image Interval of 3 seconds when the camera detects motion or heat, the first image will be collected (e.g., at 09:00:00), the second image will be collected 3 seconds later (09:00:03), and the third will be collected 3 seconds after that (09:00:06).</p>
<p>This setting differs from the Quiet Period in that the delay occurs between images contained within a multi-image sequence, rather than between multi-image sequences (as in Quiet Period). If a Motion Image Interval was not set, enter “0” seconds (i.e., instantaneous).</p>
<p id="mods-negative-binomial">Negative binomial (NB) regression (Mullahy, 1986)</p>
<p>A regression model used for count data with overdispersion but without zero-inflation. [relative abundance indices]</p>
<p id="mods-n-mixture">N-mixture models</p>
<p>A class of models for estimating absolute abundance using replicated counts of animals from several different sites; site-specific counts are treated as independent random variables to estimate the number of animals available for capture at each site; detection is imperfect (Royle 2004). N-mixture models are a type of site-structured model (i.e., that “treat each camera as though it samples… [a] distinct population within a larger meta-population” [Clarke et al., 2023]).</p>
<p id="northing-camera-location"><strong>Northing Camera Location</strong></p>
<p>The northing UTM coordinate of the camera location (e.g., “5962006”). Record using the NAD83 datum. Leave blank if recording the Latitude instead.</p>
<p id="occupancy">Occupancy</p>
<p>The probability a site is occupied by the species.</p>
<p id="mods-occupancy">Occupancy model (MacKenzie et al., 2002)</p>
<p>A modelling approach used to account for imperfect detection by first evaluating the detection probability of a species via detection histories (i.e., present or absent) to determine the probability of the true presence or absence of a species at a site (MacKenzie et al., 2002).</p>
<p id="mods-overdispersion">Overdispersion</p>
<p>A variance significantly larger than the mean (Bliss &amp; Fisher, 1953); greater variability in a set of data than predicted by the error structure of the model (Harrison et al., 2018); excess variability can be caused by zero inflation, non-independence of counts, or both (Zuur et al., 2009).</p>
<p id="sampledesign-paired">Paired design</p>
<p>A form of “clustered design” where two cameras that are placed closely together to increase detection probability (“paired cameras”), to evaluate certain conditions (“paired sites”, e.g., on- or off trails), etc. Paired placements can help to account for other variability that might occur (i.e., variation in habitat quality). For some objectives, pairs of cameras might be considered subsamples within another sampling design (e.g., simple random, stratified random, systematic).</p>
<p id="typeid-partially-marked">Partially marked individuals / populations / species</p>
<p>Individuals, populations, or species (varies with modelling approach and context) that have a suite of partially identifying traits (e.g., antler points, sex class, age class). For populations/species, those in which a proportion of individuals carry marks or in which individuals themselves are partially marked.</p>
<p id="settings-photos-per-trigger"><strong>Photos Per Trigger</strong></p>
<p>The camera setting that describes the number of photos taken each time the camera is triggered.</p>
<p id="mods-poisson">Poisson regression</p>
<p>A regression model for count data used when data are not overdispersed or zero-inflated (Lambert, 1992). [relative abundance indices]</p>
<p id="project">Project</p>
<p>A scientific study, inventory or monitoring program that has a certain objective, defined methods, and a defined boundary in space and time (recorded as “Project Name”).</p>
<p id="project-coordinator"><strong>Project Coordinator</strong></p>
<p>The first and last name of the primary contact for the project.</p>
<p id="project-coordinator-email"><strong>Project Coordinator Email</strong></p>
<p>The email address of the Project Coordinator.</p>
<p id="project-description"><strong>Project Description</strong></p>
<p>A description of the project objective(s) and general methods.</p>
<p id="project-name"><strong>Project Name</strong></p>
<p>A unique alphanumeric identifier for each project. Ideally, the Project Name should include an abbreviation for the organization, a brief project name, and the year the project began (e.g., “uofa_oilsands_2018”).</p>
<p id="pseudoreplication">Pseudoreplication</p>
<p>When observations are not statistically independent (spatially or temporally) but are treated as if they are independent.</p>
<p id="purpose-of-visit"><strong>Purpose of Visit</strong></p>
<p>The reason for visiting the camera location (i.e. to deploy the camera [“Deployment”], retrieve the camera [“Retrieve”] or to change batteries/SD card or replace the camera [“Service”]).</p>
<p id="settings-quiet-period"><strong>Quiet Period (seconds)</strong></p>
<p>The user-defined camera setting which provides the time (in seconds) between shutter “triggers” if the camera was programmed to pause between firing initially and firing a second time. If a Quiet Period was not set, enter “0.”</p>
<p>Also known as “time lag” (depending on the Camera Make and Camera Model; Palmer et al., 2018). The Quiet Period differs from the Motion Image Interval in that the delay occurs between multi-image sequences rather than between the images contained within multi-image sequences (as in the Motion Image Interval).</p>
<p id="sampledesign-random">Random (or “simple random”) design</p>
<p>Cameras occur at randomized camera locations (or sample stations) across the area of interest, sometimes with a predetermined minimum distance between camera locations (or sample stations).</p>
<p id="mods-rest">Random encounter and staying time (REST) model (Nakashima et al., 2018)</p>
<p>A recent modification of the REM (Nakashima et al., 2018) that substitutes staying time (i.e., the cumulative time in the cameras’ detection zone) for movement speed (staying time and movement speed are inversely proportional) (Cappelle et al., 2021).</p>
<p id="mods-rem">Random encounter model (REM) (Rowcliffe et al., 2008, 2013)</p>
<p>A method used to estimate the density of unmarked populations; uses the rate of independent captures, an estimate of movement rate, average group size, and the area sampled by the remote camera.</p>
<p id="recovery-time">Recovery time</p>
<p>The time necessary for the camera to prepare to capture the next photo after the previous one has been recorded (Trolliet et al., 2014).</p>
<p id="fov-registration-area">Registration area</p>
<p>The area in which an animal entering has at least some probability of being captured on the image.</p>
<p id="mods-relative-abundance">Relative abundance indices</p>
<p>An index of relative abundance. When observational data is converted to a detection rate (i.e., the frequency [count] of independent detections of a species within a distinct time period). An index can be a count of animals or any sign that is expected to vary with population size (Caughley, 1977; O’Brien, 2011).</p>
<p id="remaining-battery-percent"><strong>*Remaining Battery (%)</strong></p>
<p>The remaining battery power (%) of batteries within a camera.</p>
<p id="mods-royle-nichols">Royle-Nichols model (Royle &amp; Nichols, 2003; MacKenzie et al., 2006)</p>
<p>A method used to estimate population abundance or density, which assumes that individuals are counted only once per sampling occasion (Royle, 2004), but that does not require all individuals to be marked. Royle-Nichols models are a type of site-structured model (i.e., that “treat each camera as though it samples… [a] distinct population within a larger meta-population” [Clarke et al., 2023]).</p>
<p id="sample-station">Sample station</p>
<p>A grouping of two or more non-independent camera locations, such as when cameras are clustered or paired (recorded as “Sample Station Name”).</p>
<p id="sample-station-name"><strong>Sample Station Name</strong></p>
<p>A sequential alphanumeric identifier for each grouping of two more non-independent camera locations (when cameras are deployed in clusters, pairs, or arrays; e.g., “ss1” in “ss1_bh1”, “ss1_bh2”, “ss1_bh3” etc.). Leave blank if not applicable.</p>
<p id="baitlure-scent-lure">Scent lure</p>
<p>Any material that draws animals closer via their sense of smell (Schlexer, 2008).</p>
<p id="sd-card-id"><strong>*SD Card ID</strong></p>
<p>The ID label on an SD card (e.g., “cmu_100”).</p>
<p id="sd-card-replaced"><strong>*SD Card Replaced</strong></p>
<p>Whether the SD card was replaced.</p>
<p id="sd-card-status"><strong>*SD Card Status (% Full)</strong></p>
<p>The remaining storage capacity on an SD card; collected during a camera service or retrieval.</p>
<p id="security"><strong>*Security</strong></p>
<p>The equipment used to secure the camera (e.g., “Security box,” “Bracket,” “Bracket + Screws,” or “None”).</p>
<p id="sequence">Sequence</p>
<p>A user-defined group of images or video clips considered as a single “detection event” (recorded as “Sequence Name”); often users choose a certain time threshold (or “inter-detection interval”) to define independent “events”; e.g., 30 minutes or 1 hour. The threshold should be recorded in the Survey Design Description).</p>
<p id="sequence-name"><strong>Sequence Name</strong></p>
<p>A unique alphanumeric identifier for a multi-image sequence. The Sequence Name should ideally consist of the Deployment Name and the names of the first and last images and videos in the sequence (separated by “_”) (i.e., “Deployment Name”,_“,img_#[name of first image in sequence],”_”,img_#[name of last image in sequence] (e.g., “bh1_22-Jul-2018_img_001-img_005”). Leave blank if not applicable.</p>
<p id="service-retrieval">Service/Retrieval</p>
<p>When a crew has gone to a location to service or retrieve a remote camera.</p>
<p id="service-retrieval-comments"><strong>*Service/Retrieval Comments</strong></p>
<p>Comments describing additional details about the service/retrieval.</p>
<p id="service-retrieval-crew"><strong>Service/Retrieval Crew</strong></p>
<p>The first and last names of the individuals who collected data during the service/retrieval visit.</p>
<p id="service-retrieval-metadata">Service/retrieval metadata</p>
<p>Metadata that should be collected each time a camera location is visited to service or retrieve a camera, including data on any change to the camera location, sampling period, and/or setting type (e.g., not baited and then baited later). The relevant metadata fields that should be collected differ when a camera is deployed vs. serviced or retrieved.</p>
<p>Refer to Appendix - Table A5 and the Camera Service/Retrieval Field Datasheet.</p>
<p id="service-retrieval-visit">Service/Retrieval visit</p>
<p>When a crew has gone to a location to service or retrieve a remote camera.</p>
<p id="sex-class"><strong>Sex Class</strong></p>
<p>The sex classification of individual(s) being categorized (e.g., “Male,” “Female,” or “Unknown”).</p>
<p id="mods-ste">Space-to-event (STE) model (Moeller et al., 2018)</p>
<p>A method used to estimate abundance or density that accounts for variable detection probability through the use of time-lapse images and is unaffected by animal movement rates (collapses sampling intervals to an instant in time, and thus estimates are unaffected by animal movement rates) (Moeller et al., 2018).</p>
<p id="spatial-autocorrelation">Spatial autocorrelation</p>
<p>The tendency for locations that are closer together to be more similar.</p>
<p id="mods-sc">Spatial count (SC) model / Unmarked spatial capture-recapture (Chandler &amp; Royle, 2013)</p>
<p>A method used to estimate the density of unmarked populations; similar to SECR (Borchers &amp; Efford, 2008; Efford, 2004; Royle &amp; Young, 2008; Royle et al., 2009); however, SC models account for individuals’ unknown identities using the spatial pattern of detections (Chandler &amp; Royle, 2013; Sun et al., 2022). SC uses trap-specific counts to estimate the location and number of activity centres to estimate density.</p>
<p id="mods-smr">Spatial mark-resight (SMR) (Chandler &amp; Royle, 2013; Sollmann et al., 2013a, 2013b)</p>
<p>A method used to estimate the density of “partially marked populations by combining… [detection] histories of marked [individuals] and counts of unmarked [individuals]” (Doran-Myers, 2018) over several occasions (Sollman et al., 2013a; Rich et al., 2014; Whittington et al., 2018). SMR models can be implemented using different statistical frameworks, including Bayesian estimation (Royle and Young, 2008; Morin et al., 2022).</p>
<p id="mods-2flankspim">Spatial partial identity model (2-flank SPIM) (Augustine et al., 2018)</p>
<p>A method used to estimate the density of partially marked populations in which the “spatial locations of where partial identity samples are captured to probabilistically resolve their complete identities” (Augustine et al., 2018). Paired sampling design is commonly used to capture both the right and left flanks of an animal to resolve individual identities (Augustine et al., 2018). 2-flank SPIM is an extension of the SCR model (Borchers &amp; Efford, 2008; Efford, 2004; Royle &amp; Young, 2008; Royle et al., 2009).</p>
<p id="mods-scr-secr">Spatially explicit capture-recapture (SECR) / Spatial capture-recapture (SCR) (Borchers &amp; Efford, 2008; Efford, 2004; Royle &amp; Young, 2008; Royle et al., 2009)</p>
<p>The SECR (or SCR) method is used to estimate the density of marked populations; an extension of traditional capture-recapture (CR; Karanth, 1995; Karanth &amp; Nichols, 1998) models (Karanth, 1995; Karanth &amp; Nichols, 1998) that explicitly accounts for camera location and animal movement (Burgar et al., 2018). SECR models use spatially referenced individual capture histories to infer where animals’ home range centres are, assuming that detection probability decreases with increasing distance between cameras and home range centres (Clarke et al., 2023). SECR models can be implemented using different statistical frameworks, including Bayesian estimation (Royle and Young, 2008; Morin et al., 2022).</p>
<p id="species"><strong>Species</strong></p>
<p>The capitalized common name of the species being categorized (“tagged”).</p>
<p id="stake-distance"><strong>*Stake Distance (m)</strong></p>
<p>The distance from the camera to a stake (in metres to the nearest 0.05 m). Leave blank if not applicable.</p>
<p id="state-variable">State variable</p>
<p>A formal measure that summarizes the state of a community or population at a particular time (Wearn &amp; Glover-Kapfer, 2017), e.g., species richness or population abundance.</p>
<p id="sampledesign-stratified">Stratified design</p>
<p>The area of interest is divided into smaller strata (e.g., habitat type, disturbance levels), and cameras are placed within each stratum (e.g., 15%, 35% and 50% of sites within high, medium, and low disturbance strata).</p>
<p id="sampledesign-stratified-random">Stratified random design</p>
<p>The area of interest is divided into smaller strata (e.g., habitat type, disturbance levels), and then a proportional random sample of sites is selected within each stratum (e.g., 15%, 35% and 50% of sites within high, medium and low disturbance strata).</p>
<p id="study-area">Study area</p>
<p>A unique research, inventory or monitoring area (spatial boundary) within a project (there may be multiple study areas within a single project) (recorded as “Study Area Name”).</p>
<p id="study-area-description"><strong>Study Area Description</strong></p>
<p>A description for each unique research or monitoring area including its location, the habitat type(s), land use(s) and habitat disturbances (where applicable).</p>
<p id="study-area-name"><strong>Study Area Name</strong></p>
<p>A unique alphanumeric identifier for each study area (e.g.,”oilsands_ref1”). If only one area was surveyed, the Project Name and Study Area Name should be the same.</p>
<p id="age-class-subadult"><strong>Subadult</strong></p>
<p>Animals older than a “Juvenile” but not yet an “Adult”; a “Subadult” may be further classified into “Young of the Year” or “Yearling.”</p>
<p id="age-class-subadult-yearling"><strong>Subadult - Yearling</strong></p>
<p>Animals approximately one year old; has lived through one winter season; between “Young of Year” and “Adult.”</p>
<p id="age-class-subadult-youngofyear"><strong>Subadult - Young of Year</strong></p>
<p>Animals less than one year old; born in the previous year’s spring, but has not yet lived through a winter season; between “Juvenile” and “Yearling.”</p>
<p id="survey">Survey</p>
<p>A unique deployment period (temporal extent) within a project (recorded as “Survey Name”).</p>
<p id="survey-design"><strong>Survey Design</strong></p>
<p>The spatial arrangement of remote cameras within the study area for an individual survey. If “Hierarchical (multiple)*”, include additional details in the Survey Design Description.</p>
<p>Note that we refer to different configurations of cameras more generally as study design and sampling design; however, the term “Survey Design” refers to study design as it applies to an individual survey. There may be multiple Survey Designs for surveys within a project; if this occurs, the Survey Design should be reported separately for each survey.</p>
<p id="survey-design-description"><strong>*Survey Design Description</strong></p>
<p>A description of any additional details about the Survey Design.</p>
<p id="survey-name"><strong>Survey Name</strong></p>
<p>A unique alphanumeric identifier for each survey period (e.g., “fortmc_001”).</p>
<p id="survey-objectives"><strong>Survey Objectives</strong></p>
<p>The specific objectives of each survey within a project, including the Target Species, the state variables (e.g., occupancy, density), and proposed modelling approach(es). Survey Objectives should be specific, measurable, achievable, relevant, and time-bound (i.e., SMART).</p>
<p id="sampledesign-systematic">Systematic design</p>
<p>Camera locations occur in a regular pattern (e.g., a grid pattern) across the study area.</p>
<p id="sampledesign-systematic-random">Systematic random design</p>
<p>Camera locations are selected using a two-stage approach. Firstly, girds are selected systematically (to occur within a regular pattern) across the study area. The location of the camera within each grid is then selected randomly.</p>
<p id="tag"><strong>Tag</strong></p>
<p>When individuals, or groups of individuals, are categorized within an image, regardless of whether the information applies to all of the individuals in the image. A single tag is applied to categorize one or more individuals with the same combination of characteristics (e.g., Adult Males displaying the same Behaviour). Conversely, multiple tags are applied when individuals in an image differ in their characteristics (e.g., an Adult and a Juvenile, all else remaining equal, are tagged separately). This could also occur for Age Class, Behaviour, Human Transport Mode/Activity, etc. Since multiple tags can occur for a single image, there may be multiple data rows for the same image (if the Event Type is at the “Tag” level).</p>
<p id="target-species"><strong>Target Species</strong></p>
<p>The common name(s) of the species that the survey was designed to detect.</p>
<p id="sampledesign-targeted">Targeted design</p>
<p>Camera locations or sample stations are placed in areas that are known or suspected to have higher activity levels (e.g., game trails, mineral licks).</p>
<p id="test-image">Test image</p>
<p>An image taken from a camera after it has been set up to provide a permanent record of the visit metadata (e.g., Sample Station Name, Camera Location Name, Deployment Name, Crew, and Deployment Start Date Time [DD-MMM-YYYY HH:MM:SS]).</p>
<p>Taking a test image can be useful to compare the information from the image to that of which was collected on the Camera Service/Retrieval Field Datasheet after retrieval and can help in reducing recording errors.</p>
<p id="test-image-taken"><strong>*Test Image Taken</strong></p>
<p>Whether a test image (i.e., an image taken from a camera after it has been set up to provide a permanent record of the visit metadata) was taken. Arm the camera, from ~5 m in front, walk towards the camera while holding the Test Image Sheet.</p>
<p id="mods-tifc">Time in front of the camera (TIFC) (Huggard, 2018; Warbington &amp; Boyce, 2020; tested in Becker et al., 2022)</p>
<p>A method used to estimate density that treats camera image data as quadrat samples (Becker et al., 2022).</p>
<p id="timelapse-image">Time-lapse image</p>
<p>Images that are taken at regular intervals (e.g., hourly or daily, on the hour). It is critical to take a minimum of one time-lapse image per day at a consistent time (e.g., 12:00 pm [noon]) to create a record of camera functionality and local environmental conditions (e.g., snow cover, plant growth, etc.). Time-lapse images may always be useful for modelling approaches that require estimation of the “viewshed” (“viewshed density estimators” such as REM or time-to-event (TTE) models; see Moeller et al., [2018] for advantages and disadvantages).</p>
<p id="mods-tte">Time-to-event (TTE) model (Moeller et al., 2018)</p>
<p>A method used to estimate abundance or density from the detection rate while accounting for animal movement rates (Moeller et al., 2018). The TTE model assumes perfect detection (though there is a model extension to account for imperfect detection that requires further testing).</p>
<p id="total-number-of-camera-days">Total number of camera days</p>
<p>The number of days that all cameras were active during the survey.</p>
<p id="trigger-event">Trigger “event”</p>
<p>An activation of the camera detector(s) that initiates the capture of a single or multiple images, or the recording of video.</p>
<p id="settings-trigger-modes"><strong>Trigger Mode(s)</strong> (camera settings)</p>
<p>The camera setting(s) that determine how the camera will triggerby motion (“Motion Image”), at set intervals (“Time-lapse image”), and/or by video (“Video”; possible with newer camera models, such as Reconyx HP2X).</p>
<p id="settings-trigger-sensitivity"><strong>Trigger Sensitivity</strong></p>
<p>The camera setting responsible for how sensitive a camera is to activation (to “triggering”) via the infrared and/or heat detectors (if applicable, e.g., Reconyx HyperFire cameras have a choice between “Low,” “Low/Med,” “Med,” “Med/High,” “High,” “Very high” and “Unknown”).</p>
<p id="trigger-speed">Trigger speed</p>
<p>The time delay necessary for the camera to shoot a photo once an animal has interrupted the infrared beam within the camera’s detection zone (Trolliet et al., 2014). Trigger speed differs from Motion Image Interval (a camera setting specified by the user) in that the trigger speed is inherent to the Camera Make and Camera Model (e.g., two different cameras, models both with a Motion Image Interval set to “no delay,” may not be able to capture images at the same speed).</p>
<p id="typeid-unmarked">Unmarked individuals / populations / species</p>
<p>Individuals, populations, or species (varies with modelling approach and context) that cannot be identified using natural or artificial markings (e.g., coat patterns, scars, tags, collars). Unmarked population models rely on supplementary data (e.g., animal movement speed) and/or assumptions as a surrogate for individual identification; that is, to distinguish between multiple detections of the same individual from detections of multiple individuals when individuals do not have unique features (Gilbert et al., 2020; Morin et al., 2022).</p>
<p id="settings-userlabel">User label</p>
<p>A label (up to 16 characters) that can be programmed in the camera’s settings, and that will be visible in the data band of all photos and videos taken by the camera (Reconyx, 2018). It is recommended that users program the Sample Station Name/Camera Location Name as the user label, which serves as a means to confirm which Sample Station Name/Camera Location Name is associated with the images/videos.</p>
<p id="utm-zone-camera-location"><strong>UTM Zone Camera Location</strong></p>
<p>The number corresponding to the Universal Transverse Mercator (UTM) grid zone where the camera was placed (e.g., “12”). UTM is a coordinate system that divides the earth into grid zones that are identified with a number (representing a width of latitude) and letter (representing the hemisphere).</p>
<p>In Alberta the UTM zones are either 11, 12, or TTM. Enter all other UTM zones in the Camera Location Comments field (e.g., zones 7-10 for British Columbia), or use Latitude and Longitude instead of UTM coordinates.</p>
<p id="settings-video-length"><strong>*Video Length (seconds)</strong></p>
<p>If applicable, describes the camera setting that specifies the minimum video duration (in seconds) that the camera will record when triggered. Leave blank if not applicable.</p>
<p id="fov-viewshed">Viewshed</p>
<p>The area visible to the camera as determined by its lens angle (in degrees) and trigger distance (Moeller et al., 2023).</p>
<p id="fov-viewshed-density-estimators">Viewshed density estimators</p>
<p>Methods used to estimate the abundance of unmarked populations from observations of animals that relate animal observations to the space directly sampled by each camera’s viewshed (Moeller et al., 2023); they result in viewshed density estimates that can be extrapolated to abundance within broader sampling frames (Gilbert et al., 2020; Moeller et al., 2023).</p>
<p id="visit">Visit</p>
<p>When a crew has gone to a location to deploy, service, or retrieve a remote camera.</p>
<p id="visit-comments"><strong>*Visit Comments</strong></p>
<p>Comments describing additional details about the deployment and/or service/retrieval visits.</p>
<p id="visit-metadata">Visit metadata</p>
<p>Metadata that should be collected each time a camera location is visited to deploy, service or retrieve a camera. Other relevant metadata fields that should be collected differ when a camera is deployed vs. serviced or retrieved. <br/>Refer to Appendix A - Table A5, Camera Deployment Field Datasheet, and Camera Service/Retrieval Field Datasheet.</p>
<p id="baitlure-visual-lure">Visual lure</p>
<p>Any material that draws animals closer via their sense of sight (Schlexer, 2008).</p>
<p id="walktest">Walktest</p>
<p>A test performed to ensure the camera height, tilt, etc., adequately captures the desired detection zone. The user will 1) activate the walktest mode, 2) attach the camera at the desired height / angle, 3) walk in front of the camera to a specified distance (i.e., the “Walktest Distance,” e.g., 5 m), and 4) wave their hand in front of the camera (usually at ground level and a chosen height [i.e., the “Walktest Height,” e.g., 0.8 m]) to determine if the camera is activating (a light on the camera will flash).</p>
<p id="walktest-complete"><strong>*Walktest Complete</strong></p>
<p>Whether a walktest was performed to ensure the camera height, tilt, etc., adequately captures the desired detection zone. The user will 1) activate the walktest mode, 2) attach the camera at the desired height / angle, 3) walk in front of the camera to a specified distance (i.e., the “Walktest Distance,” e.g., 5 m), and 4) wave their hand in front of the camera (usually at ground level and a chosen height [i.e., the “Walktest Height,” e.g., 0.8 m]) to determine if the camera is activating (a light on the camera will flash).</p>
<p id="walktest-distance"><strong>Walktest Distance (m)</strong></p>
<p>The horizontal distance from the camera at which the crew performs the walktest (metres; to the nearest 0.05 m). Leave blank if not applicable.</p>
<p id="walktest-height"><strong>Walktest Height (m)</strong></p>
<p>The vertical distance from the camera at which the crew performs the walktest (metres; to the nearest 0.05 m). Leave blank if not applicable.</p>
<p id="mods-zinb">Zero-inflated negative binomial (ZINB) regression (McCullagh &amp; Nelder, 1989)</p>
<p>A regression model used in the setting of excess zeros (zero-inflation) and overdispersion. This approach is a two-part model, where the zero-inflation is modelled separately from the counts and assumes that the count (abundance) is “conditional” on the zero-inflation model (occurrence) model. [relative abundance indices]</p>
<p id="mods-zip">Zero-inflated Poisson (ZIP) regression (Lambert, 1992)</p>
<p>A regression model for count data that both follows the Poisson distribution and contains excess zeros (Lambert, 1992). ZIP models are only appropriate for data for which the overdispersion is not solely due to zero-inflation. [relative abundance indices]
Zero-inflation</p>
<p id="mods-zero-inflation">Zero-inflation</p>
<p>An excess of zeros that is “so large that those expected in standard distributions (e.g., normal, Poisson, binomial, negative binomial and beta)” (Heilbron, 1994) violate the assumptions of such distributions (Martin et al., 2005). Excess zeroes can be a result of ecological effects (“true” zeros) or due to sampling or observer error (“false zeros”) (Martin et al., 2005). Excess zeroes contribute to overdispersion, but they don’t necessarily account for all excess variability (Blasco-Moreno et al., 2019).</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./09_gloss_ref"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              
              
              
              
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="component-author">
By Alberta Remote Camera Steering Committee (RCSC)
</p>
</div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>