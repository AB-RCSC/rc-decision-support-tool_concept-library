{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1470761f-76f5-4707-9761-c4312f3a0f03",
   "metadata": {},
   "source": [
    "## Spatial capture-recapture (SCR) */ Spatially explicit capture recapture (SECR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfc68d-adc5-4dd0-80d5-a81120660309",
   "metadata": {},
   "source": [
    "```````{div} full-width\n",
    "\n",
    "``````{admonition} Click here for more information\n",
    ":class: dropdown\n",
    "\n",
    "\n",
    "later\n",
    "\n",
    "``````\n",
    "\n",
    "```````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9034638-c8c8-4399-9340-92cc65cd2cfa",
   "metadata": {},
   "source": [
    "(#i_mod_scr_secr)=\n",
    "\n",
    "### Spatial capture-recapture (SCR) */ Spatially explicit capture recapture (SECR)\n",
    "\n",
    "**{{ term_mod_scr_secr }}:** {{ term_def_scr_secr }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cf257-173e-4d53-9934-47b7d4df36eb",
   "metadata": {},
   "source": [
    ":::::{dropdown} **Assumptions, Pros, Cons**\n",
    "::::{grid} 3\n",
    ":::{grid-item-card}  **Assumptions**\n",
    "-\tDemographic closure (i.e., no births or deaths) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tDetection probability of different individuals is equal {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   or, for SECR, individuals have equal detection probability at a given distance from the centre of their home range {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tDetections of different individuals are independent {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tBehaviour is unaffected by cameras and marking {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tIndividuals do not lose marks {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tIndividuals are not misidentified {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tSurveys are independent {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tFor conventional models, geographic closure (i.e., no immigration or emigration) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tSpatially explicit models have further assumptions about animal movement {{Wearn & Glover-Kapfer, 2017; Rowcliffe et al., 2008; Royle et al., 2009; O’Brien et al., 2011}}; these include:\n",
    "    -   Home ranges are stable {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   Movement is unaffected by cameras {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   Camera locations are randomly placed with respect to the distribution and orientation of home ranges {{Wearn & Glover-Kapfer, 2017}}\n",
    "Distribution of home range centres follows a defined distribution (Poisson, or other, e.g., negative binomial) {{Wearn & Glover-Kapfer, 2017}}  \n",
    "-\tDemographic closure (i.e., no births or deaths) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tDetection probability of different individuals is equal {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   or, for SECR, individuals have equal detection probability at a given distance from the centre of their home range {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tDetections of different individuals are independent {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tBehaviour is unaffected by cameras and marking {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tIndividuals do not lose marks {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tIndividuals are not misidentified {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tSurveys are independent {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tFor conventional models, geographic closure (i.e., no immigration or emigration) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tSpatially explicit models have further assumptions about animal movement {{Wearn & Glover-Kapfer, 2017; Rowcliffe et al., 2008; Royle et al., 2009; O’Brien et al., 2011}}; these include:\n",
    "    -   Home ranges are stable {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   Movement is unaffected by cameras {{Wearn & Glover-Kapfer, 2017}}\n",
    "    -   Camera locations are randomly placed with respect to the distribution and orientation of home ranges {{Wearn & Glover-Kapfer, 2017}}\n",
    "Distribution of home range centres follows a defined distribution (Poisson, or other, e.g., negative binomial) {{Wearn & Glover-Kapfer, 2017}}  \n",
    ":::\n",
    "\n",
    ":::{grid-item-card}  **Pros** \n",
    "-\tProduces direct estimates of density or population size for explicit spatial regions {{Chandler & Royle, 2013}}\n",
    "-\tAllows researchers to mark a subset of the population/to take advantage of natural markings {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tEstimates are fully comparable across space, time, species and studies {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\t Density estimates obtained in a single model, fully incorporate spatial information of locations and individuals {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tBoth likelihood-based and Bayesian versions of the model have been implemented in relatively easy-to-use software (DENSITY and SPACECAP, respectively, as well as associated R packages) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tFlexibility in study design (e.g., \"holes\" in the trapping grid) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\t\"Open\" SECR {{Efford, 2004; Borchers & Efford, 2008; Royle & Young, 2008; Royle et al., 2009}} models exist that allow for estimation of recruitment and survival rates {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\t\"Avoid ad-hoc definitions of study area and edge effects\" {{Doran-Myers, 2018}}\n",
    "-\tSECR {{Efford, 2004; Borchers & Efford, 2008; Royle & Young, 2008; Royle et al., 2009}} accounts for variation in individual detection probability; can produce spatial variation in density; SECR {{Efford, 2004; Borchers & Efford, 2008; Royle & Young, 2008; Royle et al., 2009}} more sensitive \"to detect moderate-to-major populations changes\" (+/-20-80%) {{Morin et al., 2022; Clarke et al., 2023}}\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}  **Cons**\n",
    "-\tRequires that individuals are identifiable {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tRequires that a minimum number of individuals are trapped (each recaptured multiple times ideally) {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tRequires that each individual is captured at a number of camera locations {{Wearn & Glover-Kapfer, 2017}}\n",
    "-\tMultiple cameras per station may be required to identify individuals; difficult to implement at large spatial scales as it requires a high density of cameras {{Morin et al., 2022; Clarke et al., 2023}}\n",
    "-\tMay not be precise enough for long-term monitoring {{Green et al., 2020}}\n",
    "-\tCameras must be close enough that animals are detected at multiple camera locations {{Wearn & Glover-Kapfer, 2017}} (may be challenging to implement at large scales as many cameras are needed)\" {{Chandler & Royle, 2013}}\n",
    "-\t½ MMDM (Mean Maximum Distance Moved) will usually lead to an underestimation of home range size and thus overestimation of density {{Parmenter et al., 2003; Noss et al., 2012; Wearn & Glover-Kapfer, 2017}}\n",
    ":::\n",
    "\n",
    "::::\n",
    ":::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b52a49-dcc7-442a-a9ad-6da75bedd1e4",
   "metadata": {},
   "source": [
    "`````{tab-set}\n",
    ":selected: true\n",
    "````{tab-item} Overview\n",
    "<br>\n",
    "Spatial capture-recapture (SCR) models are used to estimate animal density. They use a combination of information \n",
    "\n",
    "SCR models rely on data in which individual animals are observed on multiple occasions, either being captured, marked and released, or individuals being identified on multiple occasions via specific diagnostic features (e.g., leopard print patterns, dolphin fin markings, etc.).\n",
    "````\n",
    "\n",
    "````{tab-item} Advanced\n",
    "<br>\n",
    "Spatial capture-recapture (SCR) models can be applied to any survey method where animals are individually identifiable and trap locations are known: live trapping and tagging, DNA sampling, camera trapping, etc. (Royle et al. 2014). Here, we will discuss camera trap SCR. \n",
    "SCR models break populations down into the activity, or home range, centres of individual animals. Let us first imagine we know the number and location of all individuals’ activity centres in a population. If we did, we could easily estimate density/: \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "```{image} ../03_images/03_image_files/clarke-et-al_2023_eqn_scr1.png\n",
    ":width: 300px\n",
    "```\n",
    "\n",
    "</center>\n",
    "\n",
    "``\r\n",
    "assuming each member of the population has an activity centre, and so the number of activity centres is equivalent to population size; and since the area encompassing all activity centres is the total area sampled by the camera array (i.e., the sampling frame; Sollmann 2018). In reality, we do not know the number and location of activity centres – indeed, the estimated number and location of activity centres is the SCR model output. \r\n",
    "\r\n",
    "To resolve the number and location of activity centres – and thus estimate density – SCR models combine information about 1) where animals are detected in space (using an observation model) and 2) how animals are distributed in space (using a spatial process moel; Figur 4; Royle 2/ \r\n",
    "\n",
    "6<r>\n",
    ")```{image} ../03_images/03_image_files/clarke-et-al_2023_fig4_clipped.png\r\n",
    ":width: 80px\r\n",
    ":align: center\r\n",
    "``\r\n",
    "\n",
    "```\n",
    "\n",
    "The observation model uses the record of where each individual was detected (i.e., individuals’ detection histories) to infer the location of each individual’s respective activity centre (Figure 5A; Chandler and Royle 2013, Royle 2016). It relies on the inverse relationship between detection probability and cameratrap-to-activity-centre distance: as the distance between a camera and an individual’s activity centre increases, the likelihood that individual will be detected there decreases (Figure 5B; Royle et al. 2014). So, animals will be detected most frequently at camera traps near their activity centres, and least frequently (or not at all) at camera traps far from their activity centres. Because the locations of activity centres are unknown, we use a spatial process model to approximate their distribution. Point-process models are a common choice (Royle 2016). A point-process model is a random pattern of points in space (Baddeley, no date); it can be homogenous (completely spatially random) or inhomogeneous (the density of points depends on landscape/habitat covariates; Royle 2016).\n",
    "\n",
    "Taken together: SCR essentially “downscales” density – a population-level estimator – to the level of the individual. The model asks: where does each animal live (Royle 2016)? Although the location of animals’ activity centres is not known, we can use information about where individuals are captured (detection histories) and how activity centres are distributed in space (point-process model) to infer where they live, and thus estimate density (Royle 2016). SCR can be implemented using many statistical frameworks, including full likelihood estimation (Borchers and Efford 2008), dataaugmented maximum likelihood estimation (Royle et al. 2014), and data-augmented Bayesian estimation (Royle and Young 2008; Morin et al. 2022). \n",
    "\n",
    "When deploying cameras for SCR analysis, practitioners must balance the area covered by the camera array with trap spacing to maximize both the number of unique individuals captured and the number of spatial recaptures of each individual. A larger sampling area will yield a higher count of unique individuals; closely-spaced traps will yield a higher number of spatial recaptures (i.e., detections of the same individual at different camera traps; Royle et al. 2014). Both are important for SCR density estimation. Cameras should also be deployed across habitat types with different levels of use (Morin et al. 2022, Sun et al. 2014). Grid and clustered sampling designs can help meet all these needs (Clark 2019, Sun et al. 2014). Note that optimal camera trap placement and spacing will change with focal species, landscape and project limitations. \n",
    "See Clark (2019), Dupont et al. (2021), Fleming et al. (2021), McFarlane et al. (2020), Nawaz et al. (2021), Romairone et al. 2018, Sollmann et al. (2012) and Sun et al.  (2014) for more detailed explorations of SCR study de```{image} ./03_images/03_image_files/clarke-et-al_2023_fig5_clipped.png\r\n",
    ":alt: clarke-et-al_2023_fig5\r\n",
    ":width: 80px\r\n",
    ":align: center\r\n",
    "```\r\n",
    "nter\n",
    "```\n",
    "\n",
    "<fo size=\"8\">\n",
    "> **{{ ref_intext_clarke-et-al_2023 }} - Figure 5.** Adapted from Morin et al. (2022) and Royle et al. (2014). A) A diagram of how the individual activity centres (circles) that make up a population might overlap with a camera array (grey crosses). The red circle highlights an example individual’s activity centre. The red arrows point towards camera stations where the red individual was detected; the numbers beside the camera stations show how many times the red individual was detected at each station. Note, the number and location of individual’s activity centres is not known, but rather inferred from the spatial pattern of detections (i.e., the number of detections of each individual at camera stations of known location). B) An example graph showing how the probability the red individual is detected at a camera station decreases with distance from its activity centre. This is reflected in A); as the distance between the red individual’s activity centre and a camera station increases, the number of detections dwindles. σ is the spatial scale parameter; it describes how detection probability decreases with increasing distance.\n",
    "</font>\n",
    "\n",
    "Another aspect of sampling design practitioners must consider is the number and configuration of cameras deployed at a station to identify animals to the individual. Left and right flanks may need to be photographed simultaneously, for example, to avoid assigning different identities to each side (Augustine et al., 2018); as another example, chest markings may need to be photographed from multiple angles at bait stations to be able to resolve identity (Proctor et al., 2022).\n",
    "````\n",
    "\n",
    "````{tab-item} Visual Resources\n",
    "\n",
    "::::{grid} 3\n",
    ":::{grid-item-card}\n",
    "```{image} ./03_images/image_files/mccomb-et-al_2010_fig10.1.png\n",
    ":width: 80px\n",
    ":align: center\n",
    "```\n",
    ":::\n",
    ":::{grid-item-card}\n",
    "```{image} ./03_images/image_files/clarke-et-al_2023_fig4_clipped.png \n",
    ":width: 80px\n",
    ":align: center\n",
    "```\n",
    ":::\n",
    ":::{grid-item-card}\n",
    "```{image} ./03_images/image_files/clarke-et-al_2023_fig5_clipped.png \n",
    ":header: \n",
    ":width: 80px\n",
    ":align: center\n",
    "```\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{grid} 3\n",
    ":::{grid-item-card}\n",
    ":header: J. Andrew Royle,\"Spatial Capture-Recapture Modelling\" ✨\n",
    ":padding: 0\n",
    ":margin: 0\n",
    ":::{iframe} https://www.youtube.com/embed/4HKFimATq9E\n",
    ":width: 100%\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":header: PAWS: Spatial Capture Recapture Data Analysis Part 1 ✨\n",
    ":::{iframe} https://www.youtube.com/embed/4HKFimATq9E\n",
    ":width: 100%\n",
    ":::\n",
    "\n",
    ":::{grid-item-card}\n",
    ":header: PAWS: Spatial Capture Recapture Data Analysis Part 2✨\n",
    ":padding: 0\n",
    ":margin: 0\n",
    ":::{iframe} https://www.youtube.com/embed/IHVez1a_hqg\n",
    ":width: 100%\n",
    ":::\n",
    "::::\n",
    "````\n",
    "\n",
    "````{tab-item} Analytical tools & resources\n",
    "\n",
    "+----------------+-----------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
    "| **Type**       | **Name**                                                                    | **Note**          | **URL**                                                                                                                                    | **Reference**                                |\n",
    "+----------------+-----------------------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------+\n",
    "\n",
    "\n",
    "````{tab-item} References\n",
    "<font size=\"1\">\n",
    "{{ ref_textbib_clarke-et-al_2023 }}\n",
    "{{ ref_textbib_efford_2024 }}\n",
    "{{ ref_textbib_gopalaswamy-et-al_2021 }}\n",
    "{{ ref_textbib_ref_efford-boulanger_2019  }}\n",
    "</font>\n",
    "````\n",
    "\n",
    "````{tab-item} Glossary\n",
    "**\\*Access Method**\n",
    ":   The method used to reach the camera location (e.g., on \"Foot,\" \"ATV,\" \"Helicopter,\" etc.).\n",
    "….\n",
    "\n",
    "</p>\n",
    "\n",
    "``\n",
    "```````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767d7de-3e12-4eff-8dcd-e91eaeea0a26",
   "metadata": {},
   "source": [
    "Spatial capture-recapture (SCR) models can be applied to any survey method where animals are individually identifiable and trap locations are known: live trapping and tagging, DNA sampling, camera trapping, etc. (Royle et al. 2014). Here, we will discuss camera trap SCR. \r\n",
    "SCR models break populations down into the activity, or home range, centres of individual animals. Let us first imagine we know the number and location of all individuals’ activity centres in a population. If we did, we could easily estimate density: \r\n",
    "\r\n",
    "```{image} ../03_images/03_image_files/clarke-et-al_2023_eqn_scr1.png\r\n",
    ":width: 80px\r\n",
    ":align: center\r\n",
    "```\r\n",
    "\r\n",
    "assuming each member of the population has an activity centre, and so the number of activity centres is equivalent to population size; and since the area encompassing all activity centres is the total area sampled by the camera array (i.e., the sampling frame; Sollmann 2018). In reality, we do not know the number and location of activity centres – indeed, the estimated number and location of activity centres is the SCR model output. \r\n",
    "\r\n",
    "To resolve the number and location of activity centres – and thus estimate density – SCR models combine information about 1) where animals are detected in space (using an observation model) and 2) how animals are distributed in space (using a spatial process model; Figure 4; Royle 201figure\n",
    "\r\n",
    "```{image} ../03_images/03_image_files/clarke-et-al_2023_fig4_clipped.png\r\n",
    ":width: 80px\r\n",
    ":align: center\r\n",
    "```\r\n",
    "\r\n",
    "The observation model uses the record of where each individual was detected (i.e., individuals’ detection histories) to infer the location of each individual’s respective activity centre (Figure 5A; Chandler and Royle 2013, Royle 2016). It relies on the inverse relationship between detection probability and cameratrap-to-activity-centre distance: as the distance between a camera and an individual’s activity centre increases, the likelihood that individual will be detected there decreases (Figure 5B; Royle et al. 2014). So, animals will be detected most frequently at camera traps near their activity centres, and least frequently (or not at all) at camera traps far from their activity centres. Because the locations of activity centres are unknown, we use a spatial process model to approximate their distribution. Point-process models are a common choice (Royle 2016). A point-process model is a random pattern of points in space (Baddeley, no date); it can be homogenous (completely spatially random) or inhomogeneous (the density of points depends on landscape/habitat covariates; Royle 2016).\r\n",
    "\r\n",
    "Taken together: SCR essentially “downscales” density – a population-level estimator – to the level of the individual. The model asks: where does each animal live (Royle 2016)? Although the location of animals’ activity centres is not known, we can use information about where individuals are captured (detection histories) and how activity centres are distributed in space (point-process model) to infer where they live, and thus estimate density (Royle 2016). SCR can be implemented using many statistical frameworks, including full likelihood estimation (Borchers and Efford 2008), dataaugmented maximum likelihood estimation (Royle et al. 2014), and data-augmented Bayesian estimation (Royle and Young 2008; Morin et al. 2022). \r\n",
    "\r\n",
    "When deploying cameras for SCR analysis, practitioners must balance the area covered by the camera array with trap spacing to maximize both the number of unique individuals captured and the number of spatial recaptures of each individual. A larger sampling area will yield a higher count of unique individuals; closely-spaced traps will yield a higher number of spatial recaptures (i.e., detections of the same individual at different camera traps; Royle et al. 2014). Both are important for SCR density estimation. Cameras should also be deployed across habitat types with different levels of use (Morin et al. 2022, Sun et al. 2014). Grid and clustered sampling designs can help meet all these needs (Clark 2019, Sun et al. 2014). Note that optimal camera trap placement and spacing will change with focal species, landscape and project limitations. \r\n",
    "See Clark (2019), Dupont et al. (2021), Fleming et al. (2021), McFarlane et al. (2020), Nawaz et al. (2021), Romairone et al. 2018, Sollmann et al. (2012) and Sun et al.  (2014) for more detailed explorations of SCfiguredy design. \r\n",
    "\r\n",
    "```{image} ../03_images/03_image_files/clarke-et-t: clarke-et-al_2023_fig5\r\n",
    ":widt \n",
    "\n",
    "ter\r\n",
    "```\r\n",
    "\r\n",
    "<font size=\"8\">\r\n",
    "> **{{ ref_intext_clarke-et-al_2023 }} - Figure 5.** Adapted from Morin et al. (2022) and Royle et al. (2014). A) A diagram of how the individual activity centres (circles) that make up a population might overlap with a camera array (grey crosses). The red circle highlights an example individual’s activity centre. The red arrows point towards camera stations where the red individual was detected; the numbers beside the camera stations show how many times the red individual was detected at each station. Note, the number and location of individual’s activity centres is not known, but rather inferred from the spatial pattern of detections (i.e., the number of detections of each individual at camera stations of known location). B) An example graph showing how the probability the red individual is detected at a camera station decreases with distance from its activity centre. This is reflected in A); as the distance between the red individual’s activity centre and a camera station increases, the number of detections dwindles. σ is the spatial scale parameter; it describes how detection probability decrth increasing distance.\r\n",
    "</font>\r\n",
    "\r\n",
    "Another aspect of sampling design practitioners must consider is the number and configuration of cameras deployed at a station to identify animals to the individual. Left and right flanks may need to be photographed simultaneously, for example, to avoid assigning different identities to each side (Augustine et al., 2018); as another example, chest markings may need to be photographed from multiple angles at bait stations to be able to resolve identity (Proctor et al., 2022).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb9fb6-d40c-4a96-8e1c-4392c075d259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}